## Feature scaling
***
* Feature scaling is a method used to normalize the range of independent variables or features of data. 
* In data processing, it is also known as data normalization and is generally performed during the data preprocessing step.
* Since the range of values of raw data varies widely, in some machine learning algorithms, objective functions will not work properly without normalization. 
* For example, many classifiers calculate the distance between two points by the Euclidean distance. 
* If one of the features has a broad range of values, the distance will be governed by this particular feature. 
* Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance.
Another reason why feature scaling is applied is that gradient descent converges much faster with feature scaling than without it


### Reference 
1. [Wiki](https://en.wikipedia.org/wiki/Feature_scaling)

## On What Data We Have to apply What  type of Scaling:
![](https://lh3.googleusercontent.com/-JgXpB4x3V0c/Xqa1jaEze1I/AAAAAAAAn5E/IF970DPGHWcMieoywvWyMAzj19drP1ywACK8BGAsYHg/s0/25.jpg)
